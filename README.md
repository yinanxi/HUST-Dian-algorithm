Dian-Algorithm-2025
===
Task 1 从头实现随机森林算法-鸢尾花分类
=
使用numpy从头实现一个随机森林算法，拟合数据集 对提供的特征的重要性进⾏评估并可视化

### 理论

**1.特征bagging：**自举过程是一种从原始样本中进行又放回的采样。在特征 bagging 过程中，我们从原始特征中进行*随机特征采样*，并且把采样到的特征传递到不同的树上面。（不采用放回的采集，因为具有冗余特征是没有意义的）。这样做事为了减少树之间的相关性。我们的目标就是制作高度不相关的决策树。![自举](C:\Users\DELL\Desktop\260a9aa590a5884d5d53e41c44009fc3.png)
![260a9aa590a5884d5d53e41c44009fc3](https://github.com/user-attachments/assets/730509af-13d0-4188-b9eb-d3886a3bb44a)
**2.聚合：**使随机森林比决策树更好的核心是**聚合不相关的树**。创建几个浅层的树模型，然后将它们*平均化*以创建更好的随机森林，这样可以将一些随机误差的平均值变为零。在回归的情况下，我们可以平均每个树的预测（平均值），而在分类问题的情况下，我们可以简单的取每个树投票的大多数类别。

#### 随机森林的原理

**Bagging（自助法采样）：**
在训练过程中，从数据集中有放回地抽取若干样本构建不同的决策树。每棵树只对一部分数据进行训练，使得模型更加稳健。

**特征随机选择：**
在每棵树的构建过程中，不是使用全部特征，而是随机选择一部分特征用于分裂节点，这进一步增强了模型的多样性。

**多数投票和平均：**

对于分类问题：多个树的预测结果通过投票决定最终类别。
对于回归问题：将所有树的输出值取平均，作为最终预测值。

用**Python**实现一个[随机森林算法](https://so.csdn.net/so/search?q=随机森林算法&spm=1001.2101.3001.7020)解决两个典型问题：分类和回归。代码将采用**面向对象的编程思想（OOP）**，通过类封装模型逻辑。
#### ID3、C4.5和CART三种经典的决策树算法

> **ID3（Iterative Dichotomiser 3）** ID3算法是决策树的基础，由Ross Quinlan提出。它基于*`信息熵和信息增益`*来选择最优特征进行划分。信息熵衡量数据集的纯度，信息增益则是在引入某一特征后数据集纯度的提升程度。ID3通常用于*分类任务*，但易受类别不平衡的影响，并且无法处理连续数值型特征。
>
> **C4.5** C4.5是ID3的升级版，同样由Ross Quinlan开发。它改进了ID3的两个主要问题：一是使用信息增益比代替信息增益，以缓解类别不平衡问题；二是能够处理连续数值型特征，通过设置阈值将其离散化。C4.5算法生成的决策树更稳定，泛化能力更强。 
>
> **CART（Classification and Regression Trees）** CART是一种`同时适用于分类和回归任务的决策树算法`。在分类任务中，CART使用`基尼不纯度`作为分裂标准；在回归任务中，使用的是`均方误差`。与ID3和C4.5不同，CART采用`二叉树结构`，这意味着每次分裂都会生成两个子节点，而非多个。这种方法简化了树结构，但也可能限制了模型的表达能力。



 在Python中实现这些算法，你需要理解以下关键步骤： 1. **数据预处理**：处理缺失值、异常值，对连续数值进行离散化，对类别型特征进行编码。 2. **计算特征选择指标（特征的重要性评判）**：对于ID3，计算信息增益；对于C4.5，计算信息增益比；对于CART，分类任务时计算基尼不纯度，回归任务时计算均方误差。 3. **选择最佳特征**：基于上一步的计算结果，选择最优特征进行划分。 4. **构建树结构**：递归地重复上述步骤，直到满足停止条件（如达到预设的最大深度、叶节点样本数小于阈值、所有特征都已尝试过等）。 5. **剪枝**：为了避免过拟合，可以对生成的树进行剪枝，如预剪枝（设定固定深度）或后剪枝（基于验证集的误差）。 6. **预测**：使用构建好的决策树对新数据进行预测。 在实际操作中，你可以使用Python的数据结构（如列表、字典、类）来存储数据、特征选择信息和决策树结构。文件`code`中可能包含了实现这些算法的代码，包括数据读取、预处理、特征选择、树构建、剪枝和预测等模块。通过阅读和理解这些代码，你可以深入了解决策树算法的内部工作原理，并可根据需求进行调整优化。同时，`scikit-learn`库提供了现成的决策树实现，可以作为对比和学习的参考。

> #### ID3信息增益算法
>
> 信息增益越大越适合


> #### **Cart算法**
>
> 基尼指数：主要用于衡量收入分配公平度的指标。在CART算法中用于衡量数据的不纯度或不确定性。基尼指数越小（MIN=0），表示数据集越纯。
> 特征分割：给定一个特征，根据该特征是或非画出两个子集
> Gini(D,a)：基于某一特征的基尼指数，需要寻找基尼指数越小的特征作为节点




> #### 信息增益比C4.5算法
> 某一特征选项越多，信息增益比的分母越大。
> 在第0层特征划分出的子集中再计算新的信息增益比。

## 剪枝

过拟合：在训练集上表现良好（mse平均均方误差=0),学到了很多细节，但在预测集上表现略弱，学到了很多错误的细节


对每个节点是否要进一步细分的选择通过计算base_SE是否大于阈值来确定，若不需要 进一步细分，则直接取细分项的加权均值


后剪枝：在完整构建完一棵树之后利用验证集剪枝，计算未合并和合并了的SE比较，决定是否剪枝

### 随机森林回归算法（CART决策树，cassification and regression tree分类与回归树）

#### 1.实现思路

**（1）选择CART回归树的训练集**：从有N个样本的样本集中，随机有放回地抽取N个样本作为当前回归树的训练样本
**（2）执行CART回归树生成算法：**
*（a）选择特征*：从数据集的M个特征中选取m个特征作为树节点的划分特征
*（b）寻找最佳切分特征m和切分点s，求解：*

遍历m个特征，对于每个特征寻找一个切分点s，求得能使上述式子达到最小值的值对（m,s），其中R1、R2为切分后的样本数据。
（c）用（b）中求得的值对（m,s）划分该节点，并满足以下条件：

并求得当前节点的输出：

（d）重复步骤（a）-（c），直到满足停止条件。
（e）最终决策树将输入样本划分成L个区域，分别对应决策树上的L个节点。决策树的非叶子节点保存当前节点的最佳切分特征和切分点，叶子节点则保存节点内所有样本的标签的均值。
（3）随机森林将多棵决策树集成起来，在预测时，随机森林中的每一棵决策树都将测试样本作为输入，每一棵决策树都会返回一个预测结果。随机森林将这些结果的平均值作为这个测试样本的预测输出。

2.并行化思路
本实验中采用了三种并行化思路：

（1）对决策树的训练使用多进程并行
这个思路比较简单。由于每棵决策树所需要的训练集都是不同的，那么可以认为每棵决策树在训练的过程中所拥有的系统资源都是独立的，互不干扰，因此可以充分使用多进程的优势而不必担心出现死锁问题。
示例代码：

（2）对决策树的预测使用多进程并行
这里和训练决策树思路相似，因为每棵决策树进行预测输出时的运算过程都是独立的，数据处理互不干扰。因此我们可以在计算每棵决策树的预测输出时为每棵决策树使用一个进程进行处理，可以加快预测速度。

（3）对决策树节点的最佳切分特征和切分点的寻找使用numpy并行计算
通过实验发现，在训练单棵决策树时，最耗时的地方就在于寻找每个节点的最佳分割特征和分割阈值。
在一般的算法中，通常使用两个for循环进行遍历：遍历所有特征，对于每个特征遍历节点内的数据集子集，找到当前特征的最佳分割阈值，最后选择损失函数最小的特征及其对应的分割阈值作为最佳分割特征与分割阈值。这种方法在小型数据集上效率还能接受，但在大数据集下运行效率极低。

于是，针对这一情况，对上述流程做出改进。通过观察可以发现，对于同一特征的每个分割阈值来说，计算切分后的方差误差这一运算是相互独立的。基于这个结论，我们可以把待遍历的所有阈值组合成一个大矩阵，即一个的矩阵，为切分阈值的数量。而对每个分割阈值进行分割操作后的输出MSE可以看作是这个矩阵按列经过一系列运算后的结果。因此我们可以将这层循环等价为一个N维矩阵的按列运算。于是，我们可以使用numpy库的矩阵运算对这部分运算进行优化，充分发挥numpy库矩阵并行化运算的优势。改进后的运算方法如下：

对于待遍历的特征列表，依次选择一个特征，重复以下操作：
① 对数据集中的训练样本根据当前特征进行排序，得到递增的分割阈值序列。
② 将排序后的数据集的y标签进行一个累加操作，得到一个数列。数列中的第n个元素代表数据集中前n个数据样本的y标签的和。
③ 将①中得到的分割阈值序列去重，得到去重后的分割阈值序列。
④ 找到中的元素在中第一次出现的位置p（从0开始算），记录数列中对应位置的元素，得到数列。中的第p-1个元素代表当前数据集以为切割阈值时，特征值小于的样本的y标签的总和，样本数为。
⑤ 对于中的每个元素，进行如下操作：用数据集中所有样本的y标签的总和减去即可得到特征值大于等于的样本的y标签的总和，样本数为。
⑥ 将数据集中所有的样本的y标签进行平方运算，得到数列，重复②③④⑤操作得到和。
⑦ 对于s’中每个元素，利用方差公式即可分别求得以当前元素（切割阈值）切割数据集后得到的两个数据集的方差。其中：

⑧ 根据⑦中的运算结果，很容易求得每个元素（切割阈值）的MSE：

⑨ 最终，我们要找的最佳分割阈值即为MSE最小值所对应的中的元素。

Task 2 Bangumi评论分数预测器(10分制）
=
说明：
==
训练框架：PyTorch；  
模型架构：**bert-base-chinsese；  

  代码结构：
  =
  数据爬取: catch_data.py；  
  数据清洗: clean.py；  
  模型训练: predict_model.py；  
  模型微调：  
        数据再清洗：finally_clean.py；  
  模型评估：evaluate_model.py；  
Hugging Face仓库链接：https://huggingface.co/YNXnanxi/Bangumi_Comment_Score_Predictor 

原理：
==
BERT采用仅包含编码器（encoder）的Transformer架构。其主要组件包括：
	1.	分词器（Tokenizer）： 将输入文本转换为整数序列（tokens）。 ￼
	2.	嵌入层（Embedding）： 将tokens映射为实值向量，包括词嵌入、位置嵌入和段嵌入。
	3.	编码器（Encoder）： 由多层Transformer块组成，利用自注意力机制捕捉上下文信息。 ￼
	4.	任务头（Task Head）： 将编码器的输出映射回词汇空间，用于特定任务的预测。
BERT通过预训练任务（如掩码语言模型和下一句预测）学习深层的双向表示，然后可以针对具体的下游任务进行微调。  

Task 3 Transformer注意力机制及其变体的理解与实现
==
#### **3-1 多头注意力机制的实现**
训练框架：PyTorch；
原理：   
**1.	线性变换生成 Q、K、V 矩阵：**  
	•	对于输入序列中的每个元素，分别通过线性变换生成查询（Query）、键（Key）和值（Value）矩阵，记为 Q、K、V。
 ￼
	•	在多头注意力机制中，这一过程会进行多次，每个头（head）都有独立的权重矩阵，用于学习不同的特征表示。   
	**2.	并行计算注意力：**  
	•	每个注意力头独立地对 Q、K、V 进行注意力计算，通常采用缩放点积注意力（Scaled Dot-Product Attention）机制。具体步骤包括：     ￼
	•	计算 Q 与 K 的转置的点积，得到注意力权重。    
	•	将上述结果除以 $\sqrt{d_k}$（ $d_k$ 为键的维度）进行缩放，防止梯度消失或爆炸。    
	•	对缩放后的结果应用 softmax 函数，获得归一化的注意力权重。    
	•	将注意力权重与 V 相乘，得到加权求和的结果。    
	**3.	合并头的输出并线性变换：**     
	•	将所有注意力头的输出拼接（concatenate）起来，形成一个新的矩阵。     ￼
	•	对拼接后的矩阵进行线性变换，得到最终的多头注意力输出。     ￼
>KV Cache
>通过缓存先前生成token的键（Key）和值（Value）对，避免重复计算，从而提升推理速度。具体而言，在Transformer模型的自注意力机制中，每个token都会生成对应的Key和Value。在生成新token时，模型可以直接使用缓存中的Key和Value，与新token的查询（Query）进行计算，生成新的输出，而无需重新计算之前所有token的Key和Value。

>Softmax函数是一种数学函数，常用于多分类问题中，将一个实数向量转换为概率分布。具体而言，Softmax函数接受一个K维向量作为输入，输出一个K维的概率分布向量，其中每个元素的取值范围在(0,1)之间，且所有元素之和为1。     ￼ ￼
>定义：  
>对于输入向量 z = [z₁, z₂, …, zₖ]，Softmax函数的第j个输出为：
>σ(z)ⱼ = e^(zⱼ) / ∑(e^(zₖ))，其中 k = 1, …, K。 ￼
>这意味着，Softmax函数对每个输入元素取指数，然后除以所有输入元素的指数和，从而将输入转换为概率分布。

#### 3-2 GQA与MQA
**1.GQA原理:** MQA 中所有头共享相同的键和值。查询向量经线性变换后分头，键值向量经线性变换后拆分并重复多头。后续计算与 MHA 类似，计算点积、缩放、掩码（若有）、SoftMax 得到注意力权重。因其共享键值，减少了内存占用和计算量。  
**2.MQA原理:** GQA 把多头注意力的头分组，每组共享键和值。查询向量线性变换后按组拆分，对每组分别计算查询与键的点积，缩放、掩码（若有）后经 SoftMax 得到组内注意力权重，最后拼接所有组的加权结果。这种方式减少了键值对的存储量。
